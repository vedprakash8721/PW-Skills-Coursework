{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Assignment Code: DA-AG-014\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zDRP4yCXRId5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Ensemble Learning in machine learning is a technique where we combine the predictions of multiple models to create a stronger overall model. Instead of relying on a single “weak” learner, ensemble methods bring together several learners to improve accuracy and robustness.\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "The central idea is that a group of models working together performs better than any single model alone.\n",
        "\n",
        "Each model (called a base learner) may make some mistakes, but when we combine their results through methods like bagging, boosting, or stacking, the errors are reduced.\n",
        "\n",
        "It works on the principle of “wisdom of the crowd” — just like asking many people for their opinion usually gives a more reliable answer than asking just one person\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jguXBbsYRRdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Bagging and Boosting are two popular ensemble learning techniques, but they work differently.\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Multiple models are trained in parallel on different random subsets of the data (sampled with replacement).\n",
        "\n",
        "Each model gives a prediction, and the final output is decided by majority vote (classification) or average (regression).\n",
        "\n",
        "Example: Random Forest.\n",
        "\n",
        "Goal: Reduce variance and avoid overfitting.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Models are trained sequentially, where each new model focuses on correcting the mistakes made by the previous ones.\n",
        "\n",
        "The final prediction is a weighted combination of all models.\n",
        "\n",
        "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "Goal: Reduce bias and improve accuracy.\n",
        "\n",
        "In short:\n",
        "\n",
        "Bagging → Parallel training, reduces variance.\n",
        "\n",
        "Boosting → Sequential training, reduces bias.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "I4_AgXvlRjYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Bootstrap sampling is a technique where we create new datasets by randomly selecting samples with replacement from the original dataset. This means the same data point can appear more than once in a sample, while some points may not appear at all.\n",
        "\n",
        "Role in Bagging (e.g., Random Forest):\n",
        "\n",
        "In Bagging, multiple models are trained on different bootstrap samples of the data.\n",
        "\n",
        "Since each sample is slightly different, each model learns different patterns.\n",
        "\n",
        "When we combine their predictions (by voting or averaging), the overall model becomes more stable and less prone to overfitting.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tnhT8bWrRxGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "- Out-of-Bag (OOB) Samples\n",
        "\n",
        "In bagging methods (like Random Forest), each tree is trained on a bootstrap sample (random sample with replacement) from the dataset.\n",
        "\n",
        "On average, about 63% of the training data is included in the bootstrap sample, leaving around 37% of the data unused for that tree.\n",
        "\n",
        "The unused data points for a given tree are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "OOB Score\n",
        "\n",
        "OOB samples act like a validation set for each tree.\n",
        "\n",
        "For every data point, we can average the predictions from only those trees for which the point was OOB.\n",
        "\n",
        "The OOB score is the accuracy (or other performance metric) calculated using these OOB predictions.\n",
        "\n",
        "Role in Model Evaluation\n",
        "\n",
        "OOB score provides an internal cross-validation mechanism, so you don’t need a separate validation set.\n",
        "\n",
        "It gives a reliable estimate of generalization error.\n",
        "\n",
        "Saves computational cost since no extra data splitting or k-fold cross-validation is required.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wVDJzha3R7xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "- Feature Importance in a Single Decision Tree vs. Random Forest\n",
        "1. In a Single Decision Tree\n",
        "\n",
        "How it works:\n",
        "\n",
        "Feature importance is measured by how much each feature reduces impurity (e.g., Gini Index or Entropy) when it is used for splitting.\n",
        "\n",
        "Each split contributes to reducing impurity. The total importance of a feature is the sum of impurity reductions from all nodes where that feature is used.\n",
        "\n",
        "This importance is then normalized (so that all features sum to 1).\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Unstable: A small change in the dataset can lead to a very different tree, changing which features appear important.\n",
        "\n",
        "Biased toward high-cardinality features: Features with many unique values (e.g., ID numbers) may appear artificially important.\n",
        "\n",
        "2. In a Random Forest\n",
        "\n",
        "How it works:\n",
        "\n",
        "Feature importance is computed across all trees in the forest.\n",
        "\n",
        "For each feature, its contribution to impurity reduction is averaged across trees.\n",
        "\n",
        "This results in more stable and reliable estimates of importance compared to a single tree.\n",
        "\n",
        "Additionally, Random Forest can also estimate feature importance using permutation importance (measuring how prediction accuracy decreases when a feature’s values are randomly shuffled).\n",
        "\n",
        "Advantages:\n",
        "\n",
        "More robust and stable (less variance due to ensemble averaging).\n",
        "\n",
        "Less biased compared to a single tree.\n",
        "\n",
        "Provides a clearer ranking of features’ contributions to predictions.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_lphuRfHSXlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "sfGNKsLdSm0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create DataFrame for feature importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and display top 5\n",
        "top_5 = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features in Breast Cancer Dataset:\")\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhMWpKtwSvOo",
        "outputId": "209d5102-5fe1-4400-b296-d60e01517703"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features in Breast Cancer Dataset:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Nf43Xhv1S2Wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "fFGcUM6CS3mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Classifier vs Single Decision Tree on Iris (version-robust)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import __version__ as skver\n",
        "\n",
        "print(\"scikit-learn version:\", skver)\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1) Single Decision Tree\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "y_pred_tree = dtree.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "# 2) Bagging with Decision Trees (handles both old/new sklearn APIs)\n",
        "base_tree = DecisionTreeClassifier(random_state=42)\n",
        "try:\n",
        "    # New API (sklearn >= 1.2)\n",
        "    bagging = BaggingClassifier(\n",
        "        estimator=base_tree,\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "except TypeError:\n",
        "    # Old API (sklearn <= 1.1)\n",
        "    bagging = BaggingClassifier(\n",
        "        base_estimator=base_tree,\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", round(tree_acc, 4))\n",
        "print(\"Accuracy of Bagging Classifier:\", round(bag_acc, 4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnijJiCFS8bS",
        "outputId": "fed898d5-f894-4a73-ef52-280e5f3408ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "Accuracy of Single Decision Tree: 0.9333\n",
            "Accuracy of Bagging Classifier: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Q31clWSPTWcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "Fmcm2f5wTYeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 4, 6, 8, None],\n",
        "    \"n_estimators\": [50, 100, 150, 200]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7Dv8daaTbex",
        "outputId": "01b82be3-9395-4d90-f37d-207fcba0695d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jEbg_vpSTlf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "H6jNmYJuTmQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Results\n",
        "print(\"Mean Squared Error - Bagging Regressor:\", mse_bagging)\n",
        "print(\"Mean Squared Error - Random Forest Regressor:\", mse_rf)\n",
        "\n",
        "if mse_rf < mse_bagging:\n",
        "    print(\"✅ Random Forest Regressor performs better.\")\n",
        "else:\n",
        "    print(\"✅ Bagging Regressor performs better.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sNw5XpGTo9o",
        "outputId": "6e39559a-dbc2-47db-c303-731c23cfbbed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error - Bagging Regressor: 0.2824242776841025\n",
            "Mean Squared Error - Random Forest Regressor: 0.2553684927247781\n",
            "✅ Random Forest Regressor performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_c4Po4rwT0Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "- Theory Part\n",
        "Bagging Regressor\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is an ensemble learning technique.\n",
        "\n",
        "It trains multiple base learners (e.g., Decision Trees) on different bootstrap samples of the dataset.\n",
        "\n",
        "Predictions are then averaged (for regression) to reduce variance and improve accuracy.\n",
        "\n",
        "Key idea: “many weak learners combined give a strong learner.”\n",
        "\n",
        "Random Forest Regressor\n",
        "\n",
        "Random Forest is an improved version of Bagging.\n",
        "\n",
        "It also trains multiple decision trees but with an extra step:\n",
        "\n",
        "At each split, it considers only a random subset of features (not all).\n",
        "\n",
        "This reduces correlation between trees and improves generalization.\n",
        "\n",
        "It is one of the most powerful ensemble methods for regression and classification."
      ],
      "metadata": {
        "id": "ujmoM1vOT0jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Bagging Regressor ---\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),   # <- FIXED: use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# --- Random Forest Regressor ---\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqw1zzEyU34o",
        "outputId": "8b88ede6-d623-47cb-e27c-2c2fc9749f5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging): 0.25592438609899626\n",
            "Mean Squared Error (Random Forest): 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZQPi706NVElQ"
      }
    }
  ]
}