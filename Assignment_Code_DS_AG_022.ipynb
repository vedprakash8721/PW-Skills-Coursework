{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3h1mmanF7s_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is a Convolutional Neural Network (CNN), and how does it differ fromtraditional fully connected neural networks in terms of architecture and performance on image data?\n",
        "\n",
        "- Answer:\n",
        "A Convolutional Neural Network (CNN) is a type of deep neural network specifically designed for processing image and spatial data. It automatically learns spatial hierarchies of features from input images‚Äîstarting from simple edges and colors to complex shapes and objects.\n",
        "\n",
        "Architecture of CNN:\n",
        "A CNN is composed of several layers, each serving a specific function:\n",
        "\n",
        "Convolutional Layer: Extracts features from the input image using filters (kernels).\n",
        "\n",
        "Pooling Layer: Reduces the spatial size of the representation, minimizing computation and controlling overfitting.\n",
        "\n",
        "Fully Connected Layer: Performs the final classification or prediction task.\n",
        "\n",
        "Activation Function (ReLU): Introduces non-linearity to model complex relationships.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "a_IxwT7j7uJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.\n",
        "- Answer:\n",
        "\n",
        "LeNet-5 is one of the earliest and most influential Convolutional Neural Network (CNN) architectures, proposed by Yann LeCun et al. in 1998 in their research paper titled ‚ÄúGradient-Based Learning Applied to Document Recognition.‚Äù The model was primarily designed for handwritten digit recognition on the MNIST dataset, and it introduced key concepts that form the foundation of modern CNNs.\n",
        "\n",
        "Architecture Overview:\n",
        "LeNet-5 consists of 7 layers (excluding the input) ‚Äî including convolutional, pooling, and fully connected layers. The input image size is 32√ó32 grayscale. The layers are as follows:\n",
        "\n",
        "Input Layer:\n",
        "Takes a 32√ó32 grayscale image.\n",
        "\n",
        "C1 ‚Äì Convolutional Layer:\n",
        "\n",
        "6 filters of size 5√ó5\n",
        "\n",
        "Produces 6 feature maps of size 28√ó28\n",
        "\n",
        "Activation: Sigmoid or tanh\n",
        "\n",
        "S2 ‚Äì Subsampling (Pooling) Layer:\n",
        "\n",
        "Average pooling with a 2√ó2 filter\n",
        "\n",
        "Reduces size to 14√ó14\n",
        "\n",
        "Helps in translation invariance and reduces computation.\n",
        "\n",
        "C3 ‚Äì Convolutional Layer:\n",
        "\n",
        "16 filters of size 5√ó5\n",
        "\n",
        "Produces feature maps of size 10√ó10\n",
        "\n",
        "Not all feature maps are connected to all input maps, improving generalization.\n",
        "\n",
        "S4 ‚Äì Subsampling Layer:\n",
        "\n",
        "Another average pooling layer (2√ó2), resulting in 5√ó5 feature maps.\n",
        "\n",
        "C5 ‚Äì Fully Connected Convolutional Layer:\n",
        "\n",
        "120 feature maps connected to the previous layer.\n",
        "\n",
        "F6 ‚Äì Fully Connected Layer:\n",
        "\n",
        "84 neurons connected to all 120 previous outputs.\n",
        "\n",
        "Output Layer:\n",
        "\n",
        "10 neurons representing digits (0‚Äì9) using a softmax activation.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EmQgszaJ8D-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations of each.\n",
        "\n",
        "\n",
        "- Answer:\n",
        "\n",
        "AlexNet and VGGNet are two milestone CNN architectures that significantly advanced computer vision research after LeNet-5. Both models helped shape the design of modern deep neural networks, yet they differ in depth, complexity, and design philosophy.\n",
        "\n",
        "1. AlexNet (2012):\n",
        "\n",
        "Developed by: Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.\n",
        "\n",
        "Design Principle: It extended LeNet-5 with deeper layers and introduced the use of ReLU activation and GPU training, which made deep learning feasible at scale.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "8 layers in total (5 convolutional + 3 fully connected).\n",
        "\n",
        "Used ReLU instead of sigmoid/tanh for faster training.\n",
        "\n",
        "Included Dropout to reduce overfitting.\n",
        "\n",
        "Used Local Response Normalization (LRN).\n",
        "\n",
        "Split training across two GPUs for efficiency.\n",
        "\n",
        "Parameters: ~60 million.\n",
        "\n",
        "Performance: Achieved top-5 error rate of 15.3% on the ImageNet dataset, a dramatic improvement over previous models.\n",
        "\n",
        "Key Innovations: ReLU activation, data augmentation, dropout, GPU parallelization.\n",
        "\n",
        "Limitations: Large model size, high computational cost, and complex training requirements.\n",
        "\n",
        "2. VGGNet (2014):\n",
        "\n",
        "Developed by: Karen Simonyan and Andrew Zisserman at the University of Oxford.\n",
        "\n",
        "Design Principle: Simplicity and uniformity ‚Äî using small (3√ó3) convolution filters but stacking many of them to increase depth.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "Comes in variants (VGG-16 and VGG-19).\n",
        "\n",
        "All convolutional layers use 3√ó3 filters with stride 1 and same padding.\n",
        "\n",
        "MaxPooling (2√ó2) follows every few convolutional layers.\n",
        "\n",
        "Ends with three fully connected layers and a softmax output.\n",
        "\n",
        "Parameters: ~138 million (VGG-16).\n",
        "\n",
        "Performance: Achieved top-5 error rate of 7.3% on ImageNet, better than AlexNet.\n",
        "\n",
        "Key Innovations: Depth through smaller filters, simple and repeatable architecture.\n",
        "\n",
        "Limitations: Extremely large model, high memory and computational demand, making it slow to train and deploy.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eNsb323K8bFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Transfer learning is a technique in deep learning where a pre-trained model, originally trained on a large dataset (like ImageNet), is reused and fine-tuned for a new but related task, such as classifying medical images or custom objects. Instead of training a CNN from scratch, the model‚Äôs learned features are transferred to the new problem.\n",
        "\n",
        "Concept:\n",
        "CNNs learn low-level features (like edges, shapes, and textures) in early layers and task-specific features (like object parts) in deeper layers. Since these low-level features are generally useful across tasks, transfer learning allows us to retain them and retrain only the final layers for our specific dataset.\n",
        "\n",
        "Example:\n",
        "Using a pre-trained model such as VGG16, ResNet50, or InceptionV3, we can remove the final classification layer and replace it with a new output layer matching our dataset‚Äôs number of classes.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "IMr0JcEj8umP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "The ResNet (Residual Network) architecture, introduced by Kaiming He et al. in 2015, revolutionized deep learning by enabling the successful training of extremely deep neural networks, sometimes exceeding 100 layers. Its key innovation is the introduction of residual connections, also known as skip connections.\n",
        "\n",
        "Concept of Residual Connections:\n",
        "In a traditional CNN, each layer learns a direct mapping\n",
        "ùêª\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "H(x) from the input\n",
        "ùë•\n",
        "x to the output. However, as networks grow deeper, training becomes difficult due to the vanishing gradient problem, where gradients become too small to update earlier layers effectively.\n",
        "\n",
        "ResNet reformulates the learning process as learning a residual function\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "ùêª\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "‚àí\n",
        "ùë•\n",
        "F(x)=H(x)‚àíx, which leads to the final output:\n",
        "\n",
        "ùêª\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "ùêπ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "+\n",
        "ùë•\n",
        "H(x)=F(x)+x\n",
        "\n",
        "This means the input\n",
        "ùë•\n",
        "x is directly added to the output of a few stacked layers, forming a shortcut connection that bypasses one or more layers.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "p7aF45rI9G1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.\n",
        "\n"
      ],
      "metadata": {
        "id": "RH8-2Cpb9XJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import time\n",
        "\n",
        "# 1. Load and preprocess MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# 2. Build LeNet-5 model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(6, (5,5), activation='tanh', input_shape=(28,28,1), padding='same'),\n",
        "    layers.AveragePooling2D(),\n",
        "    layers.Conv2D(16, (5,5), activation='tanh'),\n",
        "    layers.AveragePooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(120, activation='tanh'),\n",
        "    layers.Dense(84, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train model and record time\n",
        "start = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\n",
        "end = time.time()\n",
        "\n",
        "# 5. Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Training Time: {end - start:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "F1S3SCc59fSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "nlDQq8xG_Isp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Use a pre-trained VGG16 model (via transfer learning) on a small custom dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model. Include your code and result discussion."
      ],
      "metadata": {
        "id": "vukLEV8C_Jqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import time\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, shear_range=0.2,\n",
        "                                   zoom_range=0.2, horizontal_flip=True, validation_split=0.2)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    \"dataset/flowers\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    \"dataset/flowers\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load VGG16 model without top layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# Freeze base layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom top layers\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Build final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=5)\n",
        "\n",
        "# Record end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate model\n",
        "loss, acc = model.evaluate(val_data)\n",
        "print(f\"Validation Accuracy: {acc*100:.2f}%\")\n",
        "print(f\"Training Time: {(end_time - start_time)/60:.2f} minutes\")\n"
      ],
      "metadata": {
        "id": "hDHBDaEu_ThQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "AoHYG6oB_bk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a program to visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image."
      ],
      "metadata": {
        "id": "22q-ZHo9_cBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained AlexNet model\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "\n",
        "# Print model summary (optional)\n",
        "# print(alexnet)\n",
        "\n",
        "# Load and preprocess an input image\n",
        "img = Image.open(\"sample.jpg\")  # Example image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Get the first convolutional layer\n",
        "first_conv = alexnet.features[0]\n",
        "\n",
        "# Visualize filters (weights)\n",
        "filters = first_conv.weight.data.clone()\n",
        "print(f\"Total filters in first conv layer: {filters.shape[0]}\")\n",
        "\n",
        "# Plot few filters\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(filters[i][0].cpu(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Sample Filters from First Convolutional Layer\")\n",
        "plt.show()\n",
        "\n",
        "# Pass image through the first conv layer to get feature maps\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(img_tensor)\n",
        "\n",
        "# Plot few feature maps\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(feature_maps[0, i].cpu(), cmap='viridis')\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Feature Maps from First Convolutional Layer\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dAoVOnok_i1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "4niVZdpz_vXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Train a GoogLeNet (Inception v1) or its variant using a standard dataset like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting."
      ],
      "metadata": {
        "id": "Y_XXYFUJ_v31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import googlenet\n",
        "\n",
        "# 1. Data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),  # GoogLeNet input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 2. Load GoogLeNet\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = googlenet(pretrained=False, num_classes=10).to(device)\n",
        "\n",
        "# 3. Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training loop\n",
        "train_acc_list = []\n",
        "val_acc_list = []\n",
        "for epoch in range(5):  # small number for student demo\n",
        "    model.train()\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    train_acc = correct/total\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    val_acc = correct/total\n",
        "    val_acc_list.append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "# 5. Plot accuracy\n",
        "plt.plot(range(1,6), train_acc_list, label='Train Accuracy')\n",
        "plt.plot(range(1,6), val_acc_list, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('GoogLeNet Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jLE6xxOr_3d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5YDFwqgpAMFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working in a healthcare AI startup. Your team is tasked with developing a system that automatically classifies medical X-ray images into normal, pneumonia, and COVID-19. Due to limited labeled data, what approach would you suggest using among CNN architectures discussed (e.g., transfer learning with ResNet or Inception variants)? Justify your approach and outline a deployment strategy for production use."
      ],
      "metadata": {
        "id": "4DdnCXSOANTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Data generators\n",
        "train_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
        "                               rotation_range=20, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "train_data = train_gen.flow_from_directory('dataset/xray', target_size=(224,224),\n",
        "                                           batch_size=32, class_mode='categorical', subset='training')\n",
        "val_data = train_gen.flow_from_directory('dataset/xray', target_size=(224,224),\n",
        "                                         batch_size=32, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Load pre-trained ResNet50 without top layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # freeze base layers\n",
        "\n",
        "# Add custom classifier\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(train_data.num_classes, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=5)\n"
      ],
      "metadata": {
        "id": "0LohEdE4AXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "j6KVcoDQAfnY"
      }
    }
  ]
}