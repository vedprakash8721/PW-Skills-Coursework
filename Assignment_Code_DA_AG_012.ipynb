{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Assignment Code: DA-AG-012\n",
        "---\n",
        "\n",
        "# Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "- Answer:\n",
        "\n",
        "A Decision Tree is a machine learning method that is often used for classification problems. It works like a flowchart where we keep asking questions about the data until we reach a final decision.\n",
        "\n",
        "At the top (root node), the tree starts with the most important question or feature.\n",
        "\n",
        "From there, the data is split into smaller groups based on answers (like yes/no or true/false).\n",
        "\n",
        "This splitting continues at different nodes until we reach the leaf nodes, which give us the final class or prediction.\n",
        "\n",
        "For example, if we want to decide whether someone will play cricket, the tree may first check the weather. If it’s overcast, the answer is “Yes”. If it’s sunny, it may then check humidity, and so on, until it reaches a decision.\n",
        "\n",
        "In short, a decision tree makes predictions by asking a series of simple questions about the features of the data and following the path of answers down to the final outcome.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yHQSt9SFIu5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- Answer:\n",
        "\n",
        "When a Decision Tree is built, it has to decide which feature to split on at each step. To do this, it uses impurity measures like Gini Impurity and Entropy. These tell us how “mixed up” the classes are in a node.\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "It measures how often a randomly chosen element would be wrongly classified if we labeled it according to the distribution of classes in that node.\n",
        "\n",
        "A pure node (where all samples belong to one class) has Gini = 0.\n",
        "\n",
        "The higher the Gini value, the more mixed the classes are.\n",
        "\n",
        "Entropy:\n",
        "\n",
        "Entropy comes from information theory and measures the uncertainty or disorder in the data.\n",
        "\n",
        "If all data belongs to one class, entropy = 0 (no disorder).\n",
        "\n",
        "If the data is evenly split between classes, entropy is high (maximum disorder).\n",
        "\n",
        "Impact on Splits:\n",
        "\n",
        "Both Gini and Entropy help the tree choose the “best” feature for splitting.\n",
        "\n",
        "The algorithm looks for the split that reduces impurity the most (called Information Gain in case of Entropy).\n",
        "\n",
        "In simple words, they guide the tree to create child nodes that are purer (more homogeneous) than the parent node, improving classification accuracy.\n",
        "\n",
        "In short:\n",
        "\n",
        "Gini Impurity focuses on misclassification probability.\n",
        "\n",
        "Entropy focuses on information gain (disorder reduction).\n",
        "\n",
        "Both help the decision tree decide the best way to split the data.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Dr11kh0yXu2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- Answer:\n",
        "\n",
        "Pruning in Decision Trees means reducing the size of the tree so that it doesn’t become too complex and overfit the data. There are two types: Pre-Pruning and Post-Pruning.\n",
        "\n",
        "Pre-Pruning (Early Stopping):\n",
        "\n",
        "In this method, we stop the tree from growing too deep while it is being built.\n",
        "\n",
        "For example, we can set rules like “stop splitting if a node has fewer than 5 samples” or “stop if the tree reaches a maximum depth.”\n",
        "\n",
        "Advantage: Saves time and resources because the tree does not grow unnecessarily large.\n",
        "\n",
        "Post-Pruning (Pruning after Full Growth):\n",
        "\n",
        "Here, we first let the tree grow fully, even if it becomes very complex.\n",
        "\n",
        "After that, we cut back some branches that do not improve accuracy or that cause overfitting.\n",
        "\n",
        "Advantage: Usually gives better accuracy because the tree first explores all possible splits before trimming.\n",
        "\n",
        "In short:\n",
        "\n",
        "Pre-Pruning = Stop early to prevent complexity.\n",
        "\n",
        "Post-Pruning = Grow fully, then cut back for better generalization.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4PeEda4jYFI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- Answer:\n",
        "\n",
        "Information Gain is a measure used in Decision Trees to decide which feature should be chosen for splitting the data. It tells us how much “information” a feature gives us about the class labels.\n",
        "\n",
        "It is calculated using Entropy.\n",
        "\n",
        "At each split, the algorithm checks how much the entropy (disorder) is reduced after dividing the dataset based on that feature.\n",
        "\n",
        "The higher the Information Gain, the better that feature is at separating the data into pure groups.\n",
        "\n",
        "Why it is important:\n",
        "\n",
        "A Decision Tree’s goal is to make nodes that are as pure as possible (where most samples belong to the same class).\n",
        "\n",
        "Information Gain helps the algorithm choose the feature that makes the child nodes purer compared to the parent node.\n",
        "\n",
        "This ensures the tree becomes more accurate and efficient in classification.\n",
        "\n",
        "In short: Information Gain = reduction in disorder after a split. It is important because it helps the tree pick the best question (feature) at each step.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nCg_jJ4qYUc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "- Answer:\n",
        "\n",
        "Real-world applications of Decision Trees:\n",
        "\n",
        "Healthcare: Used to diagnose diseases based on patient symptoms and test results.\n",
        "\n",
        "Finance: Helps banks decide whether to approve a loan by analyzing income, credit history, and other factors.\n",
        "\n",
        "Marketing: Predicts customer behavior, such as whether they will respond to a campaign or buy a product.\n",
        "\n",
        "Fraud Detection: Identifies suspicious transactions by checking patterns in financial data.\n",
        "\n",
        "Education: Predicts student performance or dropout risk based on attendance, grades, and activities.\n",
        "\n",
        "Main Advantages:\n",
        "\n",
        "Easy to understand and interpret (works like asking questions).\n",
        "\n",
        "Handles both numerical and categorical data.\n",
        "\n",
        "Requires little data preprocessing (no need for normalization or scaling).\n",
        "\n",
        "Main Limitations:\n",
        "\n",
        "Can overfit if the tree is too deep.\n",
        "\n",
        "Sensitive to small changes in data (a small change may create a different tree).\n",
        "\n",
        "Less effective when there are too many features compared to advanced models (like Random Forests).\n",
        "\n",
        "In short: Decision Trees are widely used in fields like healthcare, finance, and marketing because they are simple and explainable, but they can become unstable and overfit if not controlled properly.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VCkXMhH-YmCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "-\n"
      ],
      "metadata": {
        "id": "zTfyga9_YxMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Decision Tree on Iris Dataset using Gini Criterion\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data      # Features\n",
        "y = iris.target    # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWdcaHWzZIFE",
        "outputId": "430c7daa-defb-40ef-a933-caf3ad15e612"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ENdfJ_7cZQgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "-\n"
      ],
      "metadata": {
        "id": "wEtM5CVWZRWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Decision Tree Classifier with max_depth vs fully-grown tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Fully grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy of fully-grown tree:\", accuracy_full)\n",
        "print(\"Accuracy of tree with max_depth=3:\", accuracy_pruned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfovZ35ZhPV",
        "outputId": "8999164d-f2c0-40db-8d99-9c093b0c5c0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.0\n",
            "Accuracy of tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zPEWT6DlZjp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "-"
      ],
      "metadata": {
        "id": "NXr4ycAXZkE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Decision Tree Regressor using California Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature Importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tBSPv3NZws3",
        "outputId": "934af412-724e-4307-d1e2-a5938da07e73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "IaHC1bUoZ_TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "-"
      ],
      "metadata": {
        "id": "c96p7d1bZ_5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Hyperparameter tuning of Decision Tree using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9keK4_7CaMR9",
        "outputId": "c600ddc5-6f27-431b-b6c8-a5e48e9b4ca6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "kg9olDM4aOlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Answer:\n",
        "\n",
        "If I were working as a data scientist on this healthcare problem, I would follow these steps:\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        "First, I would check how much data is missing.\n",
        "\n",
        "For numerical features, I could fill missing values using the mean or median.\n",
        "\n",
        "For categorical features, I could fill them with the most frequent category (mode) or use a special label like “Unknown.”\n",
        "\n",
        "If a feature has too many missing values (e.g., more than 40–50%), I might consider dropping it.\n",
        "\n",
        "2. Encode the Categorical Features\n",
        "\n",
        "Decision Trees can handle categorical features, but most implementations (like scikit-learn) need them in numeric form.\n",
        "\n",
        "For this, I would use One-Hot Encoding (creating dummy variables) or Label Encoding depending on the type of categorical data.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "I would split the dataset into training and testing sets.\n",
        "\n",
        "Then, I’d train a Decision Tree Classifier on the training data.\n",
        "\n",
        "The model will learn patterns from patient information (age, symptoms, test results, etc.) to predict whether the patient has the disease.\n",
        "\n",
        "4. Tune its Hyperparameters\n",
        "\n",
        "To avoid overfitting, I would tune parameters like:\n",
        "\n",
        "max_depth (how deep the tree grows),\n",
        "\n",
        "min_samples_split (minimum samples required to split a node),\n",
        "\n",
        "criterion (Gini or Entropy).\n",
        "\n",
        "I’d use GridSearchCV or RandomizedSearchCV for systematic tuning.\n",
        "\n",
        "5. Evaluate its Performance\n",
        "\n",
        "I’d test the model on the unseen test set.\n",
        "\n",
        "Metrics I’d use:\n",
        "\n",
        "Accuracy (overall correctness),\n",
        "\n",
        "Precision (how many predicted positives are truly positive),\n",
        "\n",
        "Recall (how many real positives we correctly detected),\n",
        "\n",
        "F1-score (balance of precision and recall).\n",
        "\n",
        "In healthcare, Recall is most important, because missing a sick patient is riskier than a false alarm.\n",
        "\n",
        "6. Business Value in Real-world Setting\n",
        "\n",
        "This model can help doctors quickly identify high-risk patients and prioritize their treatment.\n",
        "\n",
        "It reduces manual workload by highlighting patterns that may not be obvious.\n",
        "\n",
        "It improves early detection of diseases, which can save lives.\n",
        "\n",
        "From a business point of view, it increases efficiency, reduces costs of unnecessary tests, and improves patient satisfaction.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d0tHXw_WaPEY"
      }
    }
  ]
}