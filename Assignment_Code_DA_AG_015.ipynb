{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZA1eqr657AbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DA-AG-015"
      ],
      "metadata": {
        "id": "6Brg0WSq7B1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "s9VAHlnu7EFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "- 1. Definition of Boosting\n",
        "\n",
        "Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (models that perform slightly better than random guessing) to build a strong learner with high predictive accuracy.\n",
        "\n",
        "The main idea is to train models sequentially, where each new model focuses on correcting the mistakes made by the previous ones.\n",
        "\n",
        "2. How Boosting Works\n",
        "\n",
        "Start with a weak learner (e.g., a shallow decision tree).\n",
        "\n",
        "Assign equal weights to all training samples.\n",
        "\n",
        "Train the model → check errors.\n",
        "\n",
        "Increase weights of misclassified samples so that the next model focuses more on the “hard-to-predict” cases.\n",
        "\n",
        "Repeat the process for several rounds.\n",
        "\n",
        "Combine all the models (weighted majority voting for classification, weighted average for regression).\n",
        "\n",
        "3. How Boosting Improves Weak Learners\n",
        "\n",
        "Error correction: Each learner tries to fix the errors of the previous learner.\n",
        "\n",
        "Focus on difficult data: Misclassified points get more attention in the next round.\n",
        "\n",
        "Weighted combination: Final model gives more importance to accurate learners.\n",
        "\n",
        "Bias reduction: By sequentially adding learners, boosting reduces bias while keeping variance controlled.\n",
        "\n",
        "4. Examples of Boosting Algorithms\n",
        "\n",
        "AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost, LightGBM, CatBoost (advanced versions)\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "SIEy87be7EtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "- AdaBoost (Adaptive Boosting):\n",
        "\n",
        "Focuses on re-weighting misclassified samples.\n",
        "\n",
        "After each weak learner (usually a decision stump) is trained, the weights of incorrectly classified samples are increased so that the next learner focuses more on these hard-to-classify points.\n",
        "\n",
        "Final model is a weighted sum of weak learners.\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "Works by minimizing a loss function using gradient descent.\n",
        "\n",
        "Each new learner is trained to predict the residual errors (gradients) of the previous learners, not just re-weighted samples.\n",
        "\n",
        "Final model is an additive ensemble of learners fitted sequentially to correct residuals.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ztEW3wYN7_02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: How does regularization help in XGBoost?\n",
        "- How Does Regularization Help in XGBoost?\n",
        "\n",
        "Regularization in XGBoost is a technique used to control the complexity of the model and prevent overfitting. In boosting algorithms, adding more trees can make the model very powerful, but it also increases the risk of fitting too closely to the training data. XGBoost handles this by introducing regularization directly into the learning process.\n",
        "\n",
        "There are two main ways regularization helps in XGBoost:\n",
        "\n",
        "Controls Model Complexity\n",
        "\n",
        "XGBoost applies penalties to complex trees, meaning that trees with too many leaves or splits are discouraged.\n",
        "\n",
        "This ensures that the model does not keep growing in a way that captures noise from the training data.\n",
        "\n",
        "Prevents Overfitting\n",
        "\n",
        "By adding restrictions, regularization makes the trees more generalizable.\n",
        "\n",
        "This helps the model perform better on unseen data rather than just memorizing the training set.\n",
        "\n",
        "Encourages Simpler, More Robust Models\n",
        "\n",
        "Instead of producing very deep and complicated trees, the algorithm is encouraged to find smaller trees that still capture meaningful patterns.\n",
        "\n",
        "This balances accuracy and generalization.\n",
        "\n",
        "Stabilizes Learning\n",
        "\n",
        "Without regularization, boosting methods may aggressively reduce errors, leading to unstable predictions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1LH12JX88Nfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "- CatBoost and Categorical Data Handling\n",
        "\n",
        "CatBoost is considered highly efficient for handling categorical data because it introduces innovative techniques that eliminate the need for extensive manual preprocessing, such as one-hot encoding or label encoding. Traditional algorithms often struggle with categorical features because they either increase dimensionality (in case of one-hot encoding) or introduce arbitrary ordering (in case of label encoding).\n",
        "\n",
        "CatBoost overcomes these challenges by using a method called \"ordered target statistics\" (ordered encoding). Instead of assigning arbitrary numeric values, CatBoost replaces each categorical value with statistics derived from the target variable, but in an ordered fashion. This prevents data leakage and reduces overfitting. Additionally, CatBoost applies random permutations of the dataset to calculate these statistics in a way that preserves training integrity.\n",
        "\n",
        "Another reason for CatBoost’s efficiency is that it natively supports categorical variables, meaning you don’t have to spend extra effort transforming them. This not only saves preprocessing time but also ensures the model leverages categorical relationships more effectively.\n",
        "\n",
        "Thus, CatBoost is efficient for categorical data because it:\n",
        "\n",
        "Avoids manual encoding like one-hot or label encoding.\n",
        "\n",
        "Uses target-based encoding in an ordered, leakage-free way.\n",
        "\n",
        "Handles high-cardinality categorical features without performance degradation.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e7Jh8x7n89Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "- Boosting techniques are generally preferred over bagging methods when the problem requires high accuracy, better handling of complex patterns, and robustness against bias. While bagging methods like Random Forest are strong in reducing variance, boosting methods such as AdaBoost, Gradient Boosting, XGBoost, and CatBoost focus on reducing both bias and variance by building models sequentially. This makes them particularly useful in scenarios where precise predictions are critical.\n",
        "\n",
        "Some real-world applications include:\n",
        "\n",
        "Finance and Risk Modeling\n",
        "\n",
        "Boosting is widely used in credit scoring, fraud detection, and loan default prediction, where even small improvements in accuracy can save significant costs. The sequential learning helps capture subtle patterns in customer behavior and transaction data.\n",
        "\n",
        "Healthcare and Medical Diagnosis\n",
        "\n",
        "In medical fields, boosting algorithms are applied for disease prediction, patient readmission risks, and drug response modeling. The ability to handle imbalanced datasets (where positive cases are rare but critical) makes boosting superior to bagging methods.\n",
        "\n",
        "Marketing and Customer Analytics\n",
        "\n",
        "Boosting is used for customer churn prediction, personalized recommendations, and targeted advertising. Since these tasks often require understanding complex interactions among features, boosting tends to outperform bagging methods.\n",
        "\n",
        "Cybersecurity and Anomaly Detection\n",
        "\n",
        "Boosting is preferred for intrusion detection systems, malware classification, and spam filtering, where high precision is required to detect rare and hidden patterns without generating too many false alarms.\n",
        "\n",
        "Natural Language Processing (NLP)\n",
        "\n",
        "In text classification tasks like sentiment analysis, spam detection, and document categorization, boosting methods capture nuanced language patterns better than bagging approaches.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OENyUCuf9Gba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, AdaBoostRegressor, BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# Classification Example\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "boost_clf = AdaBoostClassifier(n_estimators=50)\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "\n",
        "boost_clf.fit(X_train, y_train)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Boosting Accuracy:\", accuracy_score(y_test, boost_clf.predict(X_test)))\n",
        "print(\"Bagging Accuracy :\", accuracy_score(y_test, bag_clf.predict(X_test)))\n",
        "\n",
        "# Regression Example\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "boost_reg = AdaBoostRegressor(n_estimators=50)\n",
        "bag_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50)\n",
        "\n",
        "boost_reg.fit(X_train, y_train)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Boosting MSE:\", mean_squared_error(y_test, boost_reg.predict(X_test)))\n",
        "print(\"Bagging MSE :\", mean_squared_error(y_test, bag_reg.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "funBtQSB9xbL",
        "outputId": "ab270646-b6e8-4038-d3ba-8a6d22ebeaa0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting Accuracy: 0.9649122807017544\n",
            "Bagging Accuracy : 0.956140350877193\n",
            "Boosting MSE: 0.8756644398099099\n",
            "Bagging MSE : 0.2579421311859296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "0v5QEcfo9-WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "# ● Print the model accuracy\n",
        "-"
      ],
      "metadata": {
        "id": "FXyneue89_NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7TB_xSy-OOM",
        "outputId": "941189e7-0074-4acd-8889-6258684e9bb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "apzPgNht-Pgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n"
      ],
      "metadata": {
        "id": "QJR_LC1b-P6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and R2 score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7GbB42a-UTj",
        "outputId": "cf7ba92e-0d15-47f4-e4a3-1cd8ce4dcdfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.7770073349512279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "T9DYVIRE-ZRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "C9lYYYmm-Z1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: XGBoost with GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Hyperparameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(estimator=xgb,\n",
        "                    param_grid=param_grid,\n",
        "                    scoring='accuracy',\n",
        "                    cv=5,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "# Fit model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMv1HhSp-rIq",
        "outputId": "28a8e05b-78ba-4357-e521-71635999e816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:12:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "syCXrzHJ-tqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n"
      ],
      "metadata": {
        "id": "TihDeqPJ-uKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: CatBoost Classifier + Confusion Matrix\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WwaO7rax_6SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "JbWNbvY2_Yej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n"
      ],
      "metadata": {
        "id": "zbUbPymY_ZLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Answer\n",
        "Start with data cleaning: handle missing values (mean/median for numeric, mode/encoding for categorical).\n",
        "\n",
        "Perform encoding for categorical features (LabelEncoder/OneHotEncoder).\n",
        "\n",
        "Apply scaling if needed (for non-tree models, not critical for boosting).\n",
        "\n",
        "Handle class imbalance using SMOTE, class weights, or scale_pos_weight.\n",
        "\n",
        "Choose boosting algorithm: XGBoost is efficient, handles missing values, and works well on tabular + imbalanced data.\n",
        "\n",
        "Split dataset into train/test using stratified sampling to maintain class balance.\n",
        "\n",
        "Perform hyperparameter tuning (GridSearchCV/RandomizedSearchCV) for learning_rate, max_depth, n_estimators.\n",
        "\n",
        "Evaluate with AUC-ROC, Precision-Recall, F1-score (since imbalance makes accuracy misleading).\n",
        "\n",
        "Interpret results with feature importance/SHAP for explainability.\n",
        "\n",
        "Business gains: better risk management, lower default rates, and higher profit margins."
      ],
      "metadata": {
        "id": "KQUyqnBh_9QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Example dataset (replace with real one)\n",
        "data = pd.DataFrame({\n",
        "    'age':[25,35,45,30,50,np.nan,40,60],\n",
        "    'income':[30000,50000,80000,40000,100000,60000,70000,120000],\n",
        "    'gender':['M','F','M','M','F','F','M','F'],\n",
        "    'default':[0,0,1,0,1,0,0,1]\n",
        "})\n",
        "\n",
        "# Handle missing values\n",
        "data['age'].fillna(data['age'].median(), inplace=True)\n",
        "\n",
        "# Encode categorical\n",
        "data['gender'] = data['gender'].map({'M':0,'F':1})\n",
        "\n",
        "# Split\n",
        "X = data.drop('default', axis=1)\n",
        "y = data['default']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
        "\n",
        "# XGBoost model\n",
        "model = XGBClassifier(scale_pos_weight=len(y[y==0])/len(y[y==1]), use_label_encoder=False, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions & Evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sng0Re1wATBb",
        "outputId": "a10629ac-2c08-4b8a-e89f-bdca32b267ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "ROC-AUC Score: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-527193551.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['age'].fillna(data['age'].median(), inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:19:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "G5NnFZvdAWbC"
      }
    }
  ]
}